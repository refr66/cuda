好的，我们继续。在前两部分，我们已经成功构建并测试了一个健壮的、运行在CPU上的`BlockManager`。现在，是时候进入GPU的世界了。

在这一部分，我们将处理真正的GPU内存，并定义 PagedAttention 中最核心的数据结构之一：**Block Table**。

---

### **第三部分：物理缓存和逻辑Block Table**

**目标：**
1.  **在GPU上分配物理KV缓存池**：我们将创建一大块连续的GPU内存，作为所有Key和Value的物理存储空间。
2.  **定义和管理Block Table**：我们将创建一个数据结构来表示每个序列的逻辑Token位置到物理内存块的映射。这是 PagedAttention 的“页表”。
3.  **整合`BlockManager`**：我们将把CPU端的`BlockManager`和GPU端的内存操作结合起来，创建一个更高层次的调度器或管理器，它知道如何为一个序列分配物理块，并更新其Block Table。

---

#### **思想过程 (Thought Process)**

1.  **物理KV缓存池应该是什么样的？**
    *   它应该是一块巨大的、连续的`torch.Tensor`，驻留在GPU上。
    *   它的维度需要仔细设计。一个直观的形状是 `(num_total_blocks, block_size, 2, num_heads, head_size)`。
        *   `num_total_blocks`：我们物理内存池中的总块数。
        *   `block_size`：每个块能存储多少个Token。这是一个关键的超参数，太小会导致Block Table过大，管理开销高；太大则会增加内部碎片（一个块没用满）。vLLM默认是16。
        *   `2`：分别用于存储Key和Value。
        *   `num_heads`：注意力头的数量。
        *   `head_size`：每个注意力头的维度。
    *   为了在CUDA Kernel中更高效地访问，我们可能会调整布局，比如 `(num_total_blocks, 2, num_heads, block_size, head_size)`。这样，属于同一个头的数据在内存中更连续。我们先采用第一种直观的布局，后续可以根据Kernel的性能需求调整。

2.  **Block Table是什么？**
    *   它本质上是一个**查找表**。对于一个长度为 `L` 的序列，它需要知道第 `0, 1, ..., L-1` 个逻辑Token的KV数据，分别存储在哪个物理块里。
    *   所以，它应该是一个张量，其长度至少等于序列的逻辑长度（按块向上取整）。
    *   **格式**：一个简单的实现是 `(max_num_sequences, max_blocks_per_sequence)` 的二维整数张量，存储在**GPU**上。
        *   `max_num_sequences`：系统能同时处理的最大请求数。
        *   `max_blocks_per_sequence`：一个序列可能占用的最大块数。
        *   `block_table[seq_id][logical_block_idx]` 的值就是这个逻辑块对应的**物理块索引 (physical_block_idx)**。
    *   **为什么在GPU上？** 因为CUDA Kernel需要直接、快速地访问这个表来进行内存寻址。如果它在CPU上，每次访存都需要昂贵的CPU-GPU数据传输，性能会崩溃。

3.  **如何将所有东西串起来？**
    *   我们需要一个更高层次的Python类，我称之为 `SequenceManager` 或 `PagedAttentionScheduler`。但为了简化，我们先在Python的`BlockManager`包装类中扩展功能，让它不仅管理块的分配，还负责维护Block Table。
    *   当一个新序列（请求）到达时：
        1.  我们为它分配一个序列ID (`seq_id`)。
        2.  我们处理它的初始prompt。假设prompt有 `N` 个Token。
        3.  我们需要 `ceil(N / block_size)` 个块。我们调用C++ `BlockManager`的 `allocate()` 方法 `ceil(N / block_size)` 次，获得一组物理块索引。
        4.  我们将这些物理块索引填充到GPU上的 `block_table` 张量的 `block_table[seq_id, ...]` 对应位置。
    *   当一个序列生成一个新Token时：
        1.  检查当前最后一个块是否已满。
        2.  如果满了，调用 `allocate()` 分配一个新块，并将其索引更新到 `block_table` 中。

4.  **数据结构管理**：
    *   我们需要在Python中跟踪每个序列的信息，比如它当前占用了哪些物理块。一个字典 `{[seq_id]: [list_of_physical_blocks]}` 是一个自然的选择。
    *   这个结构将作为CPU端的“影子”，反映GPU上 `block_table` 的状态，便于管理和调试。

---

#### **第三段代码：扩展BlockManager并引入GPU资源**

我们将修改 `mini_paged_attn/block_manager.py`，赋予它管理GPU资源和Block Table的能力。我们将这个新类命名为 `PagedAttentionManager` 以示区别。

**文件: `mini_paged_attn/manager.py`** (新文件)
这个文件将包含我们新的高层管理器。

```python
import torch
from typing import Dict, List

# 导入我们之前创建的底层 C++ 管理器
from ._C import BlockManager as CppBlockManager

class PagedAttentionManager:
    """
    Manages the entire lifecycle of sequences for PagedAttention.
    
    This class is responsible for:
    1.  Interacting with the C++ BlockManager for physical block allocation.
    2.  Allocating and managing the physical KV cache tensor on the GPU.
    3.  Creating and updating the block tables for each sequence.
    """
    
    def __init__(self,
                 num_sequences: int,
                 num_heads: int,
                 head_size: int,
                 block_size: int,
                 num_blocks: int,
                 device: str = 'cuda'):
        """
        Initializes the manager.

        Args:
            num_sequences: Maximum number of sequences to be managed.
            num_heads: Number of attention heads.
            head_size: Dimension of each attention head.
            block_size: Number of tokens per block.
            num_blocks: Total number of physical blocks.
            device: The device to store tensors on.
        """
        self.num_sequences = num_sequences
        self.num_heads = num_heads
        self.head_size = head_size
        self.block_size = block_size
        self.num_blocks = num_blocks
        self.device = device

        # 1. 初始化底层的 C++ 块管理器
        self.cpp_block_manager = CppBlockManager(num_blocks)

        # 2. 分配物理 KV 缓存池
        # Shape: (num_blocks, block_size, 2, num_heads, head_size)
        # 2 is for Key and Value
        self.kv_cache = torch.empty(
            (num_blocks, self.block_size, 2, self.num_heads, self.head_size),
            dtype=torch.float16,  # 通常使用 float16 节省显存
            device=self.device
        )
        print(f"Allocated KV Cache: {self.kv_cache.nbytes / 1024**3:.2f} GB")

        # 3. 跟踪每个序列的逻辑块到物理块的映射 (在 CPU 端)
        # key: sequence_id, value: list of physical block indices
        self.seq_block_mapping: Dict[int, List[int]] = {}

        # 4. 计算并分配 GPU 上的 Block Table
        # 假设一个序列不会超过 num_blocks 个块 (这是一个宽松的上限)
        self.max_blocks_per_seq = num_blocks 
        self.block_table = torch.zeros(
            (num_sequences, self.max_blocks_per_seq),
            dtype=torch.int32,
            device=self.device
        )

    def add_sequence(self, seq_id: int, num_prompt_tokens: int):
        """
        Allocates blocks for a new sequence's prompt.
        """
        if seq_id in self.seq_block_mapping:
            raise ValueError(f"Sequence {seq_id} already exists.")
        
        num_required_blocks = (num_prompt_tokens + self.block_size - 1) // self.block_size
        if num_required_blocks > self.cpp_block_manager.get_num_free_blocks():
            raise MemoryError("Not enough free blocks to schedule the sequence.")
        
        # 分配物理块
        physical_blocks = []
        for _ in range(num_required_blocks):
            block_idx = self.cpp_block_manager.allocate()
            physical_blocks.append(block_idx)
            
        # 更新 CPU 端的映射
        self.seq_block_mapping[seq_id] = physical_blocks
        
        # 更新 GPU 上的 Block Table
        self.block_table[seq_id, :num_required_blocks] = torch.tensor(
            physical_blocks, dtype=torch.int32, device=self.device
        )
        print(f"Added seq {seq_id} with {num_prompt_tokens} tokens, using blocks: {physical_blocks}")

    def append_token(self, seq_id: int, current_seq_len: int):
        """
        Handles appending a new token to a sequence. Allocates a new block if needed.
        """
        if seq_id not in self.seq_block_mapping:
            raise ValueError(f"Sequence {seq_id} not found.")

        # 新 Token 是第 current_seq_len 个 (从0开始)
        # 如果它刚好是一个新块的开始
        if current_seq_len % self.block_size == 0:
            print(f"Seq {seq_id} needs a new block for token {current_seq_len}.")
            try:
                new_block_idx = self.cpp_block_manager.allocate()
            except RuntimeError:
                raise MemoryError(f"Out of memory when allocating new block for seq {seq_id}.")
            
            # 更新 CPU 映射和 GPU Block Table
            logical_block_idx = current_seq_len // self.block_size
            if logical_block_idx >= self.max_blocks_per_seq:
                # 在此简化实现中，我们假设不会超过预设的最大值
                raise ValueError("Sequence exceeds max_blocks_per_seq.")

            self.seq_block_mapping[seq_id].append(new_block_idx)
            self.block_table[seq_id, logical_block_idx] = new_block_idx
            print(f"Allocated new block {new_block_idx} for seq {seq_id}")


    def free_sequence(self, seq_id: int):
        """
        Frees all blocks associated with a sequence.
        """
        if seq_id not in self.seq_block_mapping:
            return
        
        physical_blocks = self.seq_block_mapping.pop(seq_id)
        for block_idx in physical_blocks:
            self.cpp_block_manager.free(block_idx)
            
        # 重置 GPU block table 中的对应行 (可选，但良好实践)
        self.block_table[seq_id, :] = 0
        print(f"Freed seq {seq_id}, blocks: {physical_blocks}")

    def get_block_table_for_seq(self, seq_id: int) -> torch.Tensor:
        """
        Returns the block table for a specific sequence.
        """
        return self.block_table[seq_id]

```

---

#### **如何使用和测试这段代码 (概念性)**

现在我们可以模拟一个简单的调度流程。

**文件: `tests/test_manager.py`**

```python
import torch
import pytest
from mini_paged_attn.manager import PagedAttentionManager

@pytest.fixture
def manager():
    # 创建一个小型的管理器用于测试
    return PagedAttentionManager(
        num_sequences=4,
        num_heads=2,
        head_size=32,
        block_size=4, # 小 block size 方便测试
        num_blocks=16
    )

def test_add_sequence(manager: PagedAttentionManager):
    # 为 seq 0 添加一个包含 7 个 token 的 prompt
    # 需要 ceil(7/4) = 2 个块
    manager.add_sequence(seq_id=0, num_prompt_tokens=7)
    
    assert 0 in manager.seq_block_mapping
    physical_blocks = manager.seq_block_mapping[0]
    assert len(physical_blocks) == 2
    
    # 检查 GPU block table 是否正确更新
    gpu_table_slice = manager.block_table[0, :2]
    cpu_table = torch.tensor(physical_blocks, device=manager.device, dtype=torch.int32)
    assert torch.equal(gpu_table_slice, cpu_table)
    
    # 检查空闲块数量
    assert manager.cpp_block_manager.get_num_free_blocks() == 16 - 2

def test_append_token(manager: PagedAttentionManager):
    # seq 0 有 3 个 token, 使用 1 个块
    manager.add_sequence(seq_id=0, num_prompt_tokens=3)
    assert len(manager.seq_block_mapping[0]) == 1
    
    # 当前长度是 3, 附加第 4 个 token (索引为3)
    # 3 % block_size(4) != 0, 不需要新块
    manager.append_token(seq_id=0, current_seq_len=3)
    assert len(manager.seq_block_mapping[0]) == 1
    
    # 当前长度是 4, 附加第 5 个 token (索引为4)
    # 4 % block_size(4) == 0, 需要一个新块
    manager.append_token(seq_id=0, current_seq_len=4)
    assert len(manager.seq_block_mapping[0]) == 2
    
    # 检查 block table 是否更新
    gpu_table_slice = manager.block_table[0, :2]
    assert gpu_table_slice[0] != 0
    assert gpu_table_slice[1] != 0
    assert gpu_table_slice[0] != gpu_table_slice[1] # 确保分配了不同的块
    
def test_free_sequence(manager: PagedAttentionManager):
    manager.add_sequence(seq_id=1, num_prompt_tokens=5) # 使用 2 个块
    initial_free_blocks = manager.cpp_block_manager.get_num_free_blocks() # 16 - 2 = 14
    
    manager.free_sequence(seq_id=1)
    
    assert 1 not in manager.seq_block_mapping
    assert manager.cpp_block_manager.get_num_free_blocks() == initial_free_blocks + 2
```

---

#### **总结与下一步**

我们已经取得了巨大的进展！我们现在拥有：
1.  一个在 GPU 上的 **物理 KV 缓存池** (`kv_cache`)。
2.  一个在 GPU 上的 **Block Table**，作为逻辑到物理的“页表”。
3.  一个高层的 **Python 管理器**，它能使用底层的 C++ `BlockManager` 来为序列分配、追加和释放块，并实时更新 GPU 上的 Block Table。

我们已经准备好了一切输入。我们有了 Query (来自模型的输入)，有了非连续存储的 Key 和 Value (在`kv_cache`中)，还有了解码这张非连续内存地图的 `block_table`。

**下一步预告：**
终极挑战来了！我们将编写 **CUDA Kernel**。这个 Kernel 将会是 PagedAttention 的心脏，它会接收 Q、KV Cache、Block Table，并高效地执行 Attention 计算。我们将从一个简单、只求正确的版本开始。