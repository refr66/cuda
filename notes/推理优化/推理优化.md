训练和推理是两个不同的场景，推理对**延迟 (Latency)** 和**吞吐量 (Throughput)** 有着更苛刻的要求。

*   **TensorRT**:
    *   **是什么**：NVIDIA 官方的**高性能深度学习推理优化器和运行时**。它不是一个简单的库，更像一个**编译器**。
    *   **它做什么**：
        1.  **图优化**: 自动进行我们上面提到的**算子融合**。
        2.  **精度校准**: 自动选择最优的精度，最大化地使用 INT8 量化。
        3.  **Kernel 自动调优**: 根据你的具体 GPU 型号、Batch Size，从内置的 Kernel 库中选择最快的一个实现。
        4.  **动态 Shape 支持**: 处理输入尺寸可变的情况。
    *   **实践中**：对于追求极致推理性能的部署场景，将训练好的模型（如从 PyTorch 导出的 ONNX 文件）通过 TensorRT 进行优化和部署是标准流程。