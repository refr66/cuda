标准的 cuBLAS 和 cuDNN 很快，但 AI 模型中充满了各种自定义的、复合的算子。

*   **算子融合 (Kernel Fusion)**：
    *   **是什么**：将多个连续的、element-wise 的操作（如 `Add` -> `ReLU` -> `Cast`）合并成一个单一的 CUDA Kernel。
    *   **为什么重要**：**极大地减少了对 Global Memory 的读写次数**。例如，一个未经融合的操作链是 `Read -> Compute -> Write -> Read -> Compute -> Write`。融合后变成 `Read -> Compute -> Compute -> Write`。这节省了大量的内存带宽，对于 Memory-bound 的操作来说，性能提升显著。
    *   **实践中**：这是 AI 编译器（如 TVM, XLA）和推理引擎（如 TensorRT）的核心优化手段之一。作为 CUDA 开发者，当遇到性能瓶颈时，手动进行算子融合是一项关键技能。

*   **低精度计算 (Low-Precision Computing)**：
    *   **FP16 / BF16 (混合精度训练)**：使用半精度浮点数进行大部分计算（尤其是 GEMM 和卷积），同时用 FP32 保存权重副本和计算梯度更新。这能：
        1.  **减少一半的内存占用和带宽需求**。
        2.  **充分利用 Tensor Cores**。
        *   需要配合**损失缩放 (Loss Scaling)** 来避免梯度下溢。
    *   **TF32 (TensorFloat-32)**：Ampere 架构引入，是 FP32 的一种“加速模式”。它有 FP32 的范围（8位指数）和 FP16 的精度（10位尾数）。在 NVIDIA 库中，对 FP32 的 GEMM 计算会自动使用 TF32 在 Tensor Core 上加速，无需修改代码即可获得性能提升。
    *   **INT8 / INT4 (量化推理)**：将训练好的 FP32 模型权重和激活值转换为 INT8/INT4。
        1.  **极大减少内存占用**。
        2.  **极大提升计算速度**（INT8 Tensor Core 吞吐量是 FP16 的两倍）。
        *   需要一个**校准 (Calibration)** 过程来确定量化参数，以最小化精度损失。
